{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenAI Gym",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [Click here for workshop instructions](https://msu-ai.notion.site/Workshop-Instructions-eb2c76481d4c4a1f92824ee5bfd80536)"
      ],
      "metadata": {
        "id": "a9dLgaKZCHfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Installation and Setup"
      ],
      "metadata": {
        "id": "4PyzdybNw_eo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make OpenAI Gym work in Google Colab, we need to install some special packages and set up an imaginary monitor, since the Google Colab servers don't have any displays. The following block of code installs everything you need automatically. So just run it, without making any changes, using the play button:"
      ],
      "metadata": {
        "id": "1gcbwO-JlnUF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXz2QSFhwe7s"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb python-opengl x11-utils\n",
        "!pip install gym==0.23.0 pyvirtualdisplay==0.2.5 pygame mujoco_py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, run the following code to set up a \"virtual display\" so that OpenAI Gym can render the environment to the screen, even though the python server doesn't have a *real* screen."
      ],
      "metadata": {
        "id": "mKmQGe0LxUNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "# Define a show_video function that will be useful later\n",
        "import io\n",
        "from base64 import b64encode\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_video(filename, width = 400):\n",
        "  with io.open(filename,\"r+b\") as file:\n",
        "    video = file.read()\n",
        "    data_url = \"data:video/webm;base64,\" + b64encode(video).decode()\n",
        "    print(filename)\n",
        "    display(HTML(f\"\"\"<video width=\"{width}\" controls autoplay><source src=\"{data_url}\" type=\"video/webm\"></video>\"\"\"))"
      ],
      "metadata": {
        "id": "bwDSBmycOFSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent! Assuming all went well, you are now ready to begin using OpenAI Gym right here in Google Colab."
      ],
      "metadata": {
        "id": "tAZ_RDr4l37l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Welcome to the Environment"
      ],
      "metadata": {
        "id": "8JeGNJs_xBv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI Gym works by providing \"environments\", which are like scenarios that you hope the computer can learn how to navigate. That's why they call it \"gym\". It's a wonderland full of many different challenges for the computer to train on.\n",
        "\n",
        "<center>\n",
        "  <img alt=\"Six different OpenAI Gym environments\" src=\"https://blog.paperspace.com/content/images/size/w1750/2020/11/openaigym.jpg\" width=\"600\" />\n",
        "  <div><i>OpenAI Gym provides many different environments in which to train.</i></div>\n",
        "</center>\n",
        "\n",
        "In particular, OpenAI Gym is designed to be a training ground for **reinforcement learning**. Reinforcement learning is generally used when an *agent* (such as the player in a video game) needs to learn how to navigate an *environment*.\n",
        "\n",
        "If you haven't seen it before, I highly recommend checking out this short video from OpenAI about using reinforcement learning to train robots to play hide-and-seek:\n",
        "\n",
        "<center>\n",
        "  <a href=\"https://youtu.be/kopoLzvh5jY\" target=\"_blank\">\n",
        "    <img src=\"https://i.imgur.com/3OVIi32.png\" src=\"Multi-Agent Hide and Seek on Youtube\" width=\"400\" />\n",
        "  </a>\n",
        "</center>\n",
        "\n",
        "In the above video, the agents are playing in a hide-and-seek *environment*. Designing these environments is tricky and can take some time, so OpenAI has kindly created \"OpenAI Gym\", an open source python package that provides a wide variety of ready-to-use environments in which to experiment.\n",
        "\n",
        "Today, we'll be using the `\"CartPole-v1\"` environment. (Although the techniques we use will be generalizable to a large number of different environments.) CartPole is a 2D physics simulation of a cart that can move left and right, with a swining pendulum attached. The goal of the agent is to drive the cart and keep the swinging pole balanced. It's a little bit like trying to balance a broom on your hand.\n",
        "\n",
        "Let's begin by exploring the environment."
      ],
      "metadata": {
        "id": "ULEpMa6bITWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, run the following code to create a `\"CartPole-v1\"` environment. (You can see the full list of environments [here](https://gym.openai.com/envs/#classic_control), but this is what we're going to work with for now. In the future, you might want to come back and try again with a different environment.)\n",
        "\n",
        "Just run this code with no changes:"
      ],
      "metadata": {
        "id": "HsN_01L7UFbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "print(env)"
      ],
      "metadata": {
        "id": "lWqc7zZymC4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Now we have the environment stored in a variable called `env`. Next, to make sure things are set up and ready to go, let's run `env.reset()`, and store the result in a variable called `observation`. More on that later."
      ],
      "metadata": {
        "id": "RE8d0snyUXWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observation = env.reset()"
      ],
      "metadata": {
        "id": "XWOMea2fwgSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay. With the environment set up, we're finally ready to view what's going on. The environment has a method called `env.render()` that displays the environment on-screen. Usually just calling `env.render()` opens a popup window showing the environment, but that won't work in Google Colab. So we have to get a bit more fancy.\n",
        "\n",
        "The following code gets the environment image as a numpy array and then uses matplotlib to display it. (If you were here for our OpenCV workshop, you can imagine how you might draw your own graphics on top of the default output.)\n",
        "\n",
        "Alright... Run the following code to render the environment as we've described:"
      ],
      "metadata": {
        "id": "ACB5VMmSUiAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(env.render(mode=\"rgb_array\"))"
      ],
      "metadata": {
        "id": "u0t9jGg4mIEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amazing! If all went well, you should see the cart with a brown pole sticking up out of it.\n",
        "\n",
        "One important thing to notice is that **every time you call `env.reset()`, it is reset slightly randomly**. Sometimes the pole is tilted slightly right. Other times it's tilted slightly left. And sometimes it's very nearly vertical, so you can't tell that it's tilted at all. The position of the cart, also varies slightly, as do the velocity values, but those are harder to see.\n",
        "\n",
        "Check that this is true by going back, re-running the code that says `observation = env.reset()`, and then re-running the rendering code. If you do this a bunch of times, you'll see that it looks slightly different each time.\n",
        "\n",
        "---\n",
        "\n",
        "Rendering the environment is awesome, but if we want to write code, we really need to understand the environment in numbers. What are the x position of the cart and the pendulum's angle? And what are the two corresponding velocities?\n",
        "\n",
        "Well, when we ran `env.reset()`, we store the result in a variable called `observation`. That variable contains all the relavent data.\n",
        "\n",
        "Run this code to view the `observation` values:"
      ],
      "metadata": {
        "id": "Yvf7-pKuVlKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(observation)"
      ],
      "metadata": {
        "id": "oqqQj-BvCutM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, `observation` contains four numbers, as expected. But these numbers aren't labelled, so we don't know what they mean! Which one is position? Velocity? Angle? Angular velocity? We don't know.\n",
        "\n",
        "This is intentional. We want the computer to learn how to balance the pole from scratch, without us telling it anything. So all it will see is the four numbers, without any explanation of what they mean or how they change over time.\n",
        "\n",
        "But for our own sake, it would be nice to know which is which.\n",
        "\n",
        "**Question:** Which number corresponds to the angle of the pendulum?\n",
        "\n",
        "**Hint:** To figure this out, you'll need to go back and reset the environment a bunch of times. After each reset, render the environment and then look at the observation values. When the pendulum is leaning right, that means its angle is positive. When it's leaning left, the angle is negative. With this information in mind, can you figure out which number is the angle?\n",
        "\n",
        "---\n",
        "\n",
        "(Scroll down for spoilers.)\n",
        "\n",
        "---\n",
        "\n",
        ".  \n",
        ".  \n",
        ".  \n",
        ".  \n",
        "\n",
        "As you probably figured out, the order is...\n",
        "0. Cart position\n",
        "1. Cart velocity\n",
        "2. **Pendulum angle**\n",
        "3. Pendulum angular velocity\n",
        "\n",
        "OpenAI gym uses the term \"observation space\" to describe the set of possible observation values. Thinking in terms of these \"spaces\" makes it possible to create reinforcement learning algorithms that work in a wide variety of environments.\n",
        "\n",
        "If you want to learn more about the possible observation values for an environment, you can use `env.observation_space`. For example, the following code uses the built-in `.sample()` method to get a random possible observation value:"
      ],
      "metadata": {
        "id": "Y91_9IlcXSFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.observation_space.sample())"
      ],
      "metadata": {
        "id": "_CKfeUplcmm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the above code multiple times and you'll get a different random sample each time.\n",
        "\n",
        "You can also find the minimum and maximum possible observation values using the code below:"
      ],
      "metadata": {
        "id": "9V1MOc0DdQab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Minimum allowed values:\", env.observation_space.low)\n",
        "print(\"Maximum allowed values:\", env.observation_space.high)"
      ],
      "metadata": {
        "id": "sdtJAUeWZMg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the cart position will always be between -4.8 and 4.8, and so on.\n",
        "\n",
        "So this \"spaces\" idea lets us represent the possible observation values. But as we continue our simulation beyond the first frame, our agent is going to do more than *observe*. We want to *act*! This is where the `action_space` comes in. Let's see what `env.action_space` is:"
      ],
      "metadata": {
        "id": "eOKaPi2ndgM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "metadata": {
        "id": "m5hbRp3NVkKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's not a lot of information here. `Discrete(2)` just means that there are two discrete possible actions (as opposed to something continuous, like all the real numbers between 0 and 1).\n",
        "\n",
        "Let's take a random sample to see what those values can be:"
      ],
      "metadata": {
        "id": "hBjksenreIAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space.sample()"
      ],
      "metadata": {
        "id": "NDuxQTTzWGWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you run the above code a few times, you'll see that the possible actions are `0` or `1`. These correspond to moving the cart **left** or moving the cart **right** respectively.\n",
        "\n",
        "In our simulation, we advance forward one frame using either `env.step(0)` or `env.step(1)`. If we pass in the `0` action, the cart will move a little bit to the left and then the pendulum will respond accordingly. And if we pass in `1`, the cart will move a little bit to the right.\n",
        "\n",
        "The following code takes a step, always moving the cart left, and then re-renders. Run it a bunch of times and watch how the cart slowly moves left (and thus the pendulum tips right):"
      ],
      "metadata": {
        "id": "-8aRNMyleYbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step forward, moving the cart to the left\n",
        "env.step(0)\n",
        "\n",
        "# Render the environment to see what changed\n",
        "plt.imshow(env.render(mode=\"rgb_array\"))"
      ],
      "metadata": {
        "id": "1Qpu2klafN3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you go long enough, the pendulum will fall completely. When that happens, run the following code to reset. Then you can go back and do it again."
      ],
      "metadata": {
        "id": "d12iiznpfnwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "id": "n4fFEtfyfzU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try resetting and then changing the above code so that instead of moving left, the cart moves right. Watch what happens in that case. (Hopefully it is exactly what you expect.)\n",
        "\n",
        "---\n",
        "\n",
        "Alright. Remember how `env.reset()` gives back an `observation` value? Well, every time we take a step, we want to update `observation` to reflect the new state of the world.\n",
        "\n",
        "Fortunately, `env.step()` also returns information that we can read. But it returns more than just `observation`. It also returns values called `reward`, `done`, and `info`.\n",
        "\n",
        "Run the following code to see how that works:"
      ],
      "metadata": {
        "id": "qWobAdm6f2qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the environment and print the results:\n",
        "observation = env.reset()\n",
        "\n",
        "print(\"Observation (from reset):\", observation)\n",
        "print()\n",
        "\n",
        "# Next, take a step and print the results:\n",
        "observation, reward, done, info = env.step(0)\n",
        "\n",
        "print(\"Observation (from step):\", observation)\n",
        "print(\"Reward (from step):\", reward)\n",
        "print(\"Done (from step):\", done)\n",
        "print(\"Info (from step):\", info)"
      ],
      "metadata": {
        "id": "vPLXcf6qWVL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the observation values change slightly after the step. (This makes sense because the cart and pendulum have moved slightly.)\n",
        "\n",
        "But we also get `reward`, `done`, and `info`. What are they? We'll talk about them in reverse order:\n",
        "- `info` provides extra information about the environment. In some of the more complicated environments that OpenAI Gym provides, this is important. But the `\"CartPole-v1\"` that we're using doesn't provide any additional information, so this is totally useless to us.\n",
        "- `done` becomes `True` when the simulation is complete. In the `\"CartPole-v1\"` environment, the simulation is done when the pole tips down too far (i.e. you lose the game). As we write more complicated code, we're supposed to do `env.reset()` as soon as `done` becomes `True`. (We haven't been doing this so far, and so you might have seen warnings earlier complaining about this.)\n",
        "\n",
        "\n",
        "And finally, `reward`. `reward` is really important, because it lies at the heart of reinforcement learning.\n",
        "\n",
        "**What is reward?** Reinforcement learning works a lot like training a dog. The \"agent\" (computer) doesn't know what to do or what the goal is, so it begins by just trying random things. (This is called *exploration*.) Then, when the computer does something good, we give it a treat. And when it does something bad, we give it a punishment. Over time, the computer remembers what works and tries to do it again. Eventually, it knows how to earn lots and lots of treats, so that's what it does. (This is called *exploitation*.) So if we can give treats and punishments based on how the computer is doing, it should learn what to do eventually.\n",
        "\n",
        "The `reward` value is the treat or punishment for the computer. A positive number is a treat (bigger is better) and a negative number is a punishment. The computer's goal is to get the biggest `reward` possible over time. In the case of `\"CartPole-v1\"`, since the goal is just to survive as long as possible, the reward is always `1.0` (a single point for each frame you stay alive). After you die, and `done` becomes `True`, then `reward` becomes `0.0`.\n",
        "\n",
        "---\n",
        "\n",
        "One last thing before we move on... Right now we are re-running the same code block over and over again, stepping the environment forward by one frame at a time. This was fine for testing, but we really want to run the entire simulation at once and then watch a video of what happened. To do this, we'll put the `env.step()` updates inside of a `while True:` loop, and then we'll `break` out of the loop whenever `done` becomes `True` (i.e. the simulation is finished).\n",
        "\n",
        "Additionally, we'll use the built-in video recorder to record what happened and then play it back after the fact.\n",
        "\n",
        "The following code does most of this, but there are a few blanks to fill in. Refer to the previous code to remember how to do it:"
      ],
      "metadata": {
        "id": "X0dnauQm8EL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "# TODO: Create the environment\n",
        "env = # ???\n",
        "\n",
        "# Create the video recorder\n",
        "video = VideoRecorder(env, \"moving-left.mp4\")\n",
        "\n",
        "# TODO: Reset the environment\n",
        "observation = # ???\n",
        "\n",
        "while True:\n",
        "  # Capture the current frame and save it in the video\n",
        "  video.capture_frame()\n",
        "  \n",
        "  # TODO: Take a step, moving the cart to the left:\n",
        "  observation, reward, done, info = # ???\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "video.close()\n",
        "env.close()\n",
        "\n",
        "# Show the recorded video to see what happened\n",
        "show_video(\"moving-left.mp4\")"
      ],
      "metadata": {
        "id": "p1eTs3Z_xyzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If all went well, you should see a very short video (less than 1 second) of the cart moving left and the pendulum tipping right. The video is extremely short because once the pendulum begins to fall, `done` becomes `True`, so the \"episode\" of the environment simulation is complete.\n",
        "\n",
        "If the computer was more successful at balancing the pendulum, the video would be longer."
      ],
      "metadata": {
        "id": "IN6l-TuXUqCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Manual strategy coding\n",
        "Just moving left constantly is a terrible strategy for balancing the pendulum. Just about the worst possible strategy, in fact. It would be much more effective if we just made random moves (left or right) each frame instead.\n",
        "\n",
        "And we can do that! Remember `env.action_space.sample()`? It returns a random possible action (so either a `0` or a `1`).\n",
        "\n",
        "Try using that function to pick a random action (left or right) on each frame, and generate a video of the computer doing that:"
      ],
      "metadata": {
        "id": "M2-7tPDhQFz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "video = VideoRecorder(env, \"before-training.mp4\")\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  video.capture_frame()\n",
        "  \n",
        "  # TODO: Choose an action (either 0 or 1) randomly\n",
        "  action = # ???\n",
        "\n",
        "  # Take a step using the chosen action\n",
        "  # (This line of code is already correct)\n",
        "  observation, reward, done, info = env.step(action)\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "video.close()\n",
        "env.close()\n",
        "\n",
        "# Show the recorded video to see what happened\n",
        "show_video(\"before-training.mp4\")"
      ],
      "metadata": {
        "id": "v4I1VjlyVauN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct (choosing a random action each frame), then it will obviously work out a bit differently each time. But if you happen to get lucky, the computer might alternate right/left/right/left for a while, and survive with the pendulum up for at least a few frames.\n",
        "\n",
        "But perhaps we can design a better strategy.\n",
        "\n",
        "**Here's an idea:** When the pendulum is tipping to the right, move right. And when the pendulum is tipping to the left, move left.\n",
        "\n",
        "Can you code this? (See [the instructions](https://msu-ai.notion.site/Workshop-Instructions-eb2c76481d4c4a1f92824ee5bfd80536#e4eea15ff7b445da819cabfdd232e88e) for hints.)"
      ],
      "metadata": {
        "id": "V6Uq8eTdVy1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "video = VideoRecorder(env, \"before-training.mp4\")\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  video.capture_frame()\n",
        "  \n",
        "  # TODO: Choose whether to move left (0) or right (1)\n",
        "  # depending on the current `observation` data\n",
        "  if # ????\n",
        "    # Move the cart left\n",
        "    action = 0\n",
        "  else:\n",
        "    # Move the cart right\n",
        "    action = 1\n",
        "\n",
        "  # Take a step using the chosen action\n",
        "  # (This line of code is already correct)\n",
        "  observation, reward, done, info = env.step(action)\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "video.close()\n",
        "env.close()\n",
        "\n",
        "# Show the recorded video to see what happened\n",
        "show_video(\"before-training.mp4\")"
      ],
      "metadata": {
        "id": "S3ja1F5iWaQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This strategy actually seems like it's close to working! The game doesn't last super long (still under 1 second, usually), but it feels like the computer's behavior is at least beginning to be intelligent.\n",
        "\n",
        "If you want to, you can try designing an even smarter strategy that takes into account the velocity of the pendulum (`observation[3]`). It is possible to hand-craft a pretty much perfect strategy this way.\n",
        "\n",
        "But what we really want is for the computer to learn a winning strategy all on its own. And this is where reinforcement learning comes in."
      ],
      "metadata": {
        "id": "om1G5_Z0XDr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Reinforcement Learning\n",
        "\n",
        "We want the computer to learn all on its own, so reinforcement learning is the right tool for the job. There are many different methods of reinforcement learning, but today we're going to apply one of the very simplest: **Q-learning**.\n",
        "\n",
        "As discussed in the presentation, Q-learning works by building a big table of \"states\" (situations you can be in) and actions, where each cell describes whether the action is a good or bad idea in the given state.\n",
        "\n",
        "But there's a problem: The table is discrete (it has a finite number of columns), but the actual observation space is continuous. The pendulum angle, for example, could be *any number* between -0.419rad and 0.419rad (which is -24deg to 24deg). We can't make a new table column for *every possible angle/velocity combination*. Then our table would be absurdly huge.\n",
        "\n",
        "The trick here is to \"discretize\" the observation space by splitting it into buckets. We could create one bucket for angles between -24deg and -16deg, then another for angles between -16deg and -8deg, and so on. Then we would have six buckets that look like this:\n",
        "\n",
        "<center>\n",
        "  <img alt=\"Splitting the possible angles into six discrete buckets\" src=\"https://i.imgur.com/bF5AJU9.png\" />\n",
        "  <div>\n",
        "    <i>We can split the possible angles into discrete buckets.</i>\n",
        "  </div>\n",
        "</center>\n",
        "\n",
        "Then we make one column for each bucket, and say that everything in the purple range, for example, is basically the same.\n",
        "\n",
        "But of course, we don't *only* care about the angle of the pendulum. The velocity of the pendulum is also important.\n",
        "\n",
        "(And in fact, `observation` *also* gives us the position and velocity of the cart. If we were being strict about forcing the computer to learn everything on its own, we would feed it all that information (and create table columns for every possible combination), and ask the computer to sift through it. But the cart position and velocity are not very important, so to make the training process (much) faster, we are going to ignore them and just build our Q table using the just angle and angular velocity.)\n",
        "\n",
        "---\n",
        "\n",
        "So we want to create buckets for angle and buckets for angular velocity. The python library `sklearn` has a tool called `KBinsDiscretizer` that does this for us, but we need to provide it with the minimum and maximum allowed values along with the number of buckets to create.\n",
        "\n",
        "What are the minimum and maximum allowed values? `env.observation_space` can tell us!"
      ],
      "metadata": {
        "id": "jR3eR_byazn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lower bounds:\", env.observation_space.low)\n",
        "print(\"Upper bounds:\", env.observation_space.high)"
      ],
      "metadata": {
        "id": "Jlup0UnpGO6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We only care about the last two values (because those correspond to the pendulum angle and angular velocity). The values for angle, -0.419 and 0.419, make sense, because that is -24 to 24 degrees. But the values for angular velocity are extremely large! ($3.4 \\cdot 10^{38}$) These bounds are telling us that, technically, there's no limit on velocity. But we can't make that many bins! And in practice, the pendulum will never swing that fast anyway. So let's just assume that the angular velocity will always be between -1.0 and 1.0 radians per second (rather than the super huge numbers we're getting).\n",
        "\n",
        "The following code creates a `KBinsDiscretizer` called `bucketer` that will split our values into buckets. But the code is not quite complete. Can you finish it?"
      ],
      "metadata": {
        "id": "24xeCtD3NTqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "# 6 buckets for angle, 12 buckets for angular velocity\n",
        "numer_of_buckets = (6, 12)\n",
        "\n",
        "# Use the lower bounds from the observation space (but use -1.0 for angular velocity)\n",
        "lower_bounds = [env.observation_space.low[2], -1.0]\n",
        "\n",
        "# TODO: Use the upper bounds from the observation space (but use 1.0 for angular velocity)\n",
        "upper_bounds = # ???\n",
        "\n",
        "# Create a KBinsDiscretizer called `bucketer` using our settings from above\n",
        "bucketer = KBinsDiscretizer(n_bins=numer_of_buckets, encode=\"ordinal\", strategy=\"uniform\")\n",
        "bucketer.fit([lower_bounds, upper_bounds])"
      ],
      "metadata": {
        "id": "EAXmcoHWMStq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully you have a `bucketer` set up with the correct bounds. Now let's try using it! The bucketer allows us to plug in an angle and an angular velocity, and get back two bucket numbers."
      ],
      "metadata": {
        "id": "V4iLjIWpOpMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bucketer.transform([[-0.415, 0.97]])"
      ],
      "metadata": {
        "id": "y4UL4GlWOuj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** What is the number corresponding to the blue bucket in the following picture? What about the green bucket?\n",
        "\n",
        "<center>\n",
        "  <img alt=\"Splitting the possible angles into six discrete buckets\" src=\"https://i.imgur.com/bF5AJU9.png\" />\n",
        "</center>"
      ],
      "metadata": {
        "id": "A2jnOpipPMBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the `bucketer`, let's create a function called `discretizer` that takes an `observation` and gives back two bucket numbers, for the angle and angular velocity.\n",
        "\n",
        "The function is almost complete. Can you finish it?"
      ],
      "metadata": {
        "id": "VhN0__QwQgeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the \"discretizer\" function\n",
        "def discretizer(observation):\n",
        "  # Get the angle value from the observation\n",
        "  angle = observation[2]\n",
        "\n",
        "  # TODO: Get the angular velocity value from the observation\n",
        "  angular_velocity = # ???\n",
        "\n",
        "  bucket_numbers = bucketer.transform([[angle, angular_velocity]])[0]\n",
        "  bucket_numbers = map(int, bucket_numbers)\n",
        "  return tuple(bucket_numbers)\n",
        "\n",
        "# Try discretizing an observation\n",
        "observation = env.reset()\n",
        "discretizer(observation)"
      ],
      "metadata": {
        "id": "fFNFNHP0Ps5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you run the above code multiple times, you should see that the angle bucket is always 2 or 3 and the angular velocity bucket is always 5 or 6.\n",
        "\n",
        "---\n",
        "\n",
        "Alright. Having these discrete buckets means we can now create our `Q_table`. So let's go ahead and do that. We'll begin by filling it with zeroes. Run this code to create the table:"
      ],
      "metadata": {
        "id": "EAl6JyD5QxzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "Q_table = np.zeros((6, 12, 2))\n",
        "\n",
        "print(Q_table)"
      ],
      "metadata": {
        "id": "3TSYQ2IpNqdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the Q-table is full of zeroes. And its size is $6 \\times 12 \\times 2$ because there are 6 angle buckets and 12 angular velocity buckets, and for each possible state (combination of angle + angular velocity), there are 2 possible actions: move right or move left.\n",
        "\n",
        "So each cell in the `Q_table` will tell us whether a particular action is a good idea in a particular state. But of course, it's all filled with zeroes right now. We still need to learn the correct values for the table.\n",
        "\n",
        "---\n",
        "\n",
        "During this learning processs, we want the computer to start out by *exploring* a lot (trying random stuff), and then eventually learn what's best and do the things it knows are a good idea (*exploitation*). So we want to have an `exploration_rate` that decreases over time.\n",
        "\n",
        "Your job is to create an `exploration_rate` function where at episode 0, `n = 0`, the exploration rate is 1, and at episode 200, `n = 200`, the exploration rate is 0.1. For now, it probably makes sense to choose a simple function like a line. Try creating your own function below, and then check that the results make sense."
      ],
      "metadata": {
        "id": "-g3lIOPFdvbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exploration_rate(n):\n",
        "  # TODO: Make the rate decrease slowly over time\n",
        "  rate = # ???\n",
        "\n",
        "  if rate < 0.1:\n",
        "    rate = 0.1\n",
        "  \n",
        "  if rate > 1.0:\n",
        "    rate = 1.0\n",
        "  \n",
        "  return rate\n",
        "\n",
        "# Check the values of the function\n",
        "print(\"Exploration rate at n = 0:\", exploration_rate(0), \"(should be 1.0)\")\n",
        "print(\"Exploration rate at n = 200:\", exploration_rate(200), \"(should be 0.1)\")\n",
        "\n",
        "# Graph the function\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot([exploration_rate(x) for x in range(1000)])\n",
        "plt.ylabel('Exploration rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q3ee2-0iOQGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully your graph looks like it's decreasing from $(0, 1.0)$ to $(200, 0.1)$ and then becoming a flat 0.1 beyond that point. (There's always room to experiment, but this is a solid starting place.)\n",
        "\n",
        "---\n",
        "\n",
        "Now, we also want the learning rate to change over time. At first, when the computer doesn't know anything, it should adapt very strongly based on any new information it sees (i.e. the learning rate should be high). But after a while, when it has played many games already, one new measurement shouldn't affect its strategy too much. So later on, the learning rate should be low.\n",
        "\n",
        "Create another function, `learning_rate`, similar to the last. It should have a learning rate of `1.0` at `n = 0`, and then it should bottom out at a learning rate of `0.01` somewhere around `n = 200`."
      ],
      "metadata": {
        "id": "QJBzgHZjgDVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decaying learning rate\n",
        "def learning_rate(n):\n",
        "  # TODO: Make the rate decrease slowly over time\n",
        "  rate = # ???\n",
        "\n",
        "  if rate < 0.01:\n",
        "    rate = 0.01\n",
        "  \n",
        "  if rate > 1.0:\n",
        "    rate = 1.0\n",
        "  \n",
        "  return rate\n",
        "\n",
        "# Check the values of the function\n",
        "print(\"Learning rate at n = 0:\", learning_rate(0), \"(should be 1.0)\")\n",
        "print(\"Learning rate at n = 200:\", learning_rate(200), \"(should be 0.01)\")\n",
        "\n",
        "# Graph the function\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot([learning_rate(x) for x in range(1000)])\n",
        "plt.ylabel('Learning rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6ZKLsBHOOKXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, hopefully your graph starts at $(0, 1.0)$ and drops down to about $(200, 0.01)$ before leveling out.\n",
        "\n",
        "---\n",
        "\n",
        "Alright... With all these helper functions in place, we're finally ready to perform Q-learning. We are going to run the environment for 1,000 episodes (remember, an episode ends when the pendulum falls). Throughout the entire process, we will update the `Q_table` continuously based on the `reward` received after each step. (To learn more about exactly how the table is updated, see the instructions document.)\n",
        "\n",
        "The following code is mostly complete, but inside the if-else statement (explore or exploit), the explore branch is *supposed* to choose a random action (0 or 1), but the code is not there. Can you fill it in? (Remember, we chose a random action from `env.action_space` previously.)\n",
        "\n",
        "Finish the code and then run it. It will take a while to complete (~3 minutes)."
      ],
      "metadata": {
        "id": "aTJnQbolhc2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will take about 3 minutes to run\n",
        "\n",
        "for episode_number in range(1000):\n",
        "  if episode_number % 25 == 0:\n",
        "    print(f\"Beginning episode {episode_number}...\")\n",
        "\n",
        "  observation = env.reset()\n",
        "  \n",
        "  while True:\n",
        "    current_state = discretizer(observation)\n",
        "\n",
        "    # Choose an action (either explore or exploit based on exploration_rate)\n",
        "    if np.random.random() < exploration_rate(episode_number):\n",
        "      # TODO: Explore (random action)\n",
        "      action = # ???\n",
        "    else:\n",
        "      # Exploit (best action according to Q_table)\n",
        "      action = np.argmax(Q_table[current_state])\n",
        "    \n",
        "    # Take a physics step in the environment\n",
        "    observation, reward, done, _ = env.step(action)\n",
        "    new_state = discretizer(observation)\n",
        "    \n",
        "    # Update Q_table\n",
        "    lr = learning_rate(episode_number)\n",
        "    old_value = Q_table[current_state][action]\n",
        "    new_value = reward + 1 * np.max(Q_table[new_state])\n",
        "    Q_table[current_state][action] = (1-lr)*old_value + lr*new_value\n",
        "\n",
        "    # If this episode is done, stop the loop so we can begin the next one\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(\"Done. Q_table is now ready to be used.\")"
      ],
      "metadata": {
        "id": "f6RSEVv4Obct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amazing! Now that the `Q_table` is built, the computer can use it to play. Let's record a video of the computer playing the game using the `Q_table` to decide which action is best:"
      ],
      "metadata": {
        "id": "Twl6YqYby2-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we've trained, play using the new Q-Table and record a video.\n",
        "import gym\n",
        "import numpy as np\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "video = VideoRecorder(env, \"after-training.mp4\")\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "for _ in range(300):\n",
        "  video.capture_frame()\n",
        "\n",
        "  # Always exploit (choose best possible action based on Q_table)\n",
        "  current_state = discretizer(observation)\n",
        "  action = np.argmax(Q_table[current_state])\n",
        "\n",
        "  observation, reward, done, _ = env.step(action)\n",
        "\n",
        "video.close()\n",
        "env.close()\n",
        "\n",
        "show_video(\"after-training.mp4\")"
      ],
      "metadata": {
        "id": "RgtNjSCcQJ-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fantastic! It works! ðŸŽ‰"
      ],
      "metadata": {
        "id": "x9hO4XrJzBOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 (Bonus): Try a New Environment\n",
        "\n",
        "If you are done early, a fantastic next challenge is to explore [a new environment](https://gym.openai.com/envs/#classic_control).\n",
        "\n",
        "1. Get the environment up and running.\n",
        "2. Manually code a strategy like we did in step 3. Can you write code that plays the game successfully?\n",
        "3. **Extra challenge:** Apply Q-learning to the new environment.\n",
        "\n",
        "If you successfully run a new environment, send a video in the Discord! ðŸ¤  (For MuJoCo installation instructions, see [the instructions document](https://msu-ai.notion.site/Workshop-Instructions-eb2c76481d4c4a1f92824ee5bfd80536#0592b475cc2940be8b348d4607dc8019).)\n",
        "\n",
        "Here is some code to get you started with `\"MountainCar-v0\"`:"
      ],
      "metadata": {
        "id": "h9-IB567GI5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "plt.imshow(env.render(mode=\"rgb_array\"))"
      ],
      "metadata": {
        "id": "suNnVYA9GVOV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}